User Managment:
==============

create a usercreds:
=================

kubectl create namespace devteam
mkdir devcreds ==> to use this directory to store user creds
openssl genrsa -out developer.key 2048
openssl req -new -key developer.key -out developer.csr -subj "/CN=developer/O=javadeveloper"
cn=commonname;O=organization
cat developer.csr
* sign this certificate with cluster ca
ls -tlh /etc/kubernetes/pki ==> this is the certificate location of kubernetes(kubeadm)
openssl x509 -req -in developer.csr -CA /etc/kuberntes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out developer.crt -days 500
developer.csr , developer.crt , developer.key ==> by using these keys user can access k8s env.

create a role for ns:
====================
rbac.yaml

kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
    namespace: javaproject
    name: deployment-manager-role
rules:
- apiGroups: ["extensions","apps"]
  resources: ["deployments","repicasets","pods"]
  verbs: ["get","list","watch","create","update","patch","delete"]

kubectl create -f rbac.yaml

role-binding:
============
rolebind.yaml

kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
    namespace: javaproject
    name: deployment-manager-rolebinding
subjects:
- kind: User
  name: developer
  apiGroup: ""
roleRef:
    kind: role
    name: deployment-manager-role
    apiGroup: ""

kubectl apply -f rolebinding.yaml 

Note: set up developer user account can communicate to kubenetes cluster machine

kubectl config set-credentials developer --client-certificate=root/developer/developer.crt --client-key=/root/developer/developer.key 

kubectl config set-context developer-context --cluster=kubernetes --namespace=javaproject --user=developer

after that we need to verify config file

cat ~/.kube/config

kubectl --kubeconfig ./admin.config config view -o jsonpath='{.contexts[*].name}'
kubectl --kubeconfig ./admin.config get pods --namespace=javaproject

liveness:
========
liveness probe is resposible for telling to k8s cluster conatiner need to be restarted because of there is a deadlock is happen in k8s cluster in that case container should not work.

livenessprobe is mentioned in conatiner section in manifest file.

livenessProbe:
  httpGet:
    path: /hello
    #health status endpoint of the application
    port: 8080
    #port on which it is running
  initialDelaySeconds: 15
  #after container is starting it will wait for 15sec after then checking    any deadlock is happening or not.
  periodSeconds: 10
  #regular interval after the application startup.

note:
====
  intials delayseconds and period seconds work same but initialdelayseconds is useful when you deployed your application first time.its gona wait for the 15 sec.
  after that they can regularly using peroidseconds.

  az aks:
  ======
  login in k8s 
  #az aks Get-Credentials --resource-group kube-rg --name cluster-1
  merged "Cluster-1" as a current context in /home/satya/.kube/config

  manage identity:
  ============
  #az aks create -g myRSG -n myManagedCluster --enable-managed-identity
  
  update existing managed identity:
  ==============================

  #az aks update -g <RGname> -n <AKSName> --enable-managed-identity --assign-dentity <UserAssignedIdentityResourceID> az aks create -g MyResourceGroup -n MyManagedCluster --enable-managed-identity
